{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coded by Chris Ha (hac541309@gmail.com) of EleutherAI, DuckAI\n",
    "# for polyglot(EleutherAI) and polylingual(DuckAI) projects\n",
    "# Licensed as MIT or Apache 2.0 or later versions of these licenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstitute into dicts\n",
    "# this might be just done with dict(tokenizer_json['model']['vocab'])\n",
    "def recover_vocab_dict(tokenizer_json: dict) -> dict:\n",
    "    tokenizer_dict = {}\n",
    "    tokenizer_list = tokenizer_json['model']['vocab']\n",
    "    for nlp_pair in tokenizer_list:\n",
    "        tokenizer_dict[nlp_pair[0]] = nlp_pair[1]\n",
    "    return tokenizer_dict\n",
    "\n",
    "def check_duplicates(lst):\n",
    "    count_dict = {}\n",
    "    for item in lst:\n",
    "        if item[0] in count_dict:\n",
    "            count_dict[item[0]] += 1\n",
    "        else:\n",
    "            count_dict[item[0]] = 1\n",
    "    duplicates = {key: value for key, value in count_dict.items() if value > 1}\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equal : averages nlp\n",
    "# min : takes the lowest (unlikeliest) of two options\n",
    "# max : takes the highest (likeliest) of two options\n",
    "nlp_mode = \"max\"\n",
    "# this epsilon value is later used to separate byte level tokens \n",
    "# They will have the lowest negative log probs, the most unlikeliest.\n",
    "# This will put them on the bottom, and other characters or tokens will be tokenized above them.\n",
    "\n",
    "epsilon = 1E-7\n",
    "# whether to bring in added_tokens of additional vocabs\n",
    "with_added_tokens = False\n",
    "\n",
    "# Since the ByteLevel works as its name suggests, at the byte level, it encodes each byte value to a unique visible character. \n",
    "# This means that there is a total of 256 different characters composing this alphabet.\n",
    "byte_level_alphabet = sorted(tokenizers.pre_tokenizers.ByteLevel.alphabet())\n",
    "assert len(byte_level_alphabet) == 256\n",
    "\n",
    "# if not None : trim to vocab_size.\n",
    "target_vocab_size = 65536\n",
    "# the number we set the internal tokens to achieve the final intended vocab size\n",
    "target_intermediate_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# everything except the vocab is inherited from base\n",
    "# special tokens, add tokens, preprocessing etc\n",
    "base_tokenizer_path = \"/home/karyo/corpus/models/merging/bulm_kr_65k.json\"\n",
    "add_tokenizer_path = \"/home/karyo/corpus/models/merging/bulm_en_32k.json\"\n",
    "assert os.path.isfile(base_tokenizer_path)\n",
    "assert os.path.isfile(add_tokenizer_path)\n",
    "new_tokenizer_path = \"/home/karyo/corpus/models/merging/final.json\"\n",
    "base_tokenizer = tokenizers.Tokenizer.from_file(base_tokenizer_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size = 65536, Unadded size = 65513\n",
      "Internal Total Size = 65536, Unadded size = 65513\n",
      "65513\n"
     ]
    }
   ],
   "source": [
    "base_total_size = base_tokenizer.get_vocab_size(True)\n",
    "base_unadded_size = base_tokenizer.get_vocab_size(False)\n",
    "print(f\"Total Size = {base_total_size}, Unadded size = {base_unadded_size}\")\n",
    "\n",
    "#internal representation might be different due to <bos> <eos>\n",
    "base_internal_total = len(base_tokenizer.get_vocab(True))\n",
    "base_internal_unadded = len(base_tokenizer.get_vocab(False))\n",
    "print(f\"Internal Total Size = {base_internal_total}, Unadded size = {base_internal_unadded}\")\n",
    "assert (base_total_size - base_unadded_size) == (base_internal_total - base_internal_unadded)\n",
    "internal_offset = base_total_size - base_internal_total\n",
    "added_offset = base_total_size - base_unadded_size\n",
    "\n",
    "# we are setting the intermediate_size according to offsets\n",
    "# the difference arises from how huggingface tokenizers treat added tokens and special tokens\n",
    "if target_vocab_size:\n",
    "    target_intermediate_size = target_vocab_size - (internal_offset + added_offset)\n",
    "    print(target_intermediate_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(base_tokenizer_path) as base_file,open(add_tokenizer_path) as add_file:\n",
    "    base_tokenizer_json = json.load(base_file)\n",
    "    add_tokenizer_json = json.load(add_file)\n",
    "    # this only works for unigram\n",
    "    assert base_tokenizer_json['model']['type'].lower() == \"unigram\"\n",
    "    assert add_tokenizer_json['model']['type'].lower() == \"unigram\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65513 32745\n"
     ]
    }
   ],
   "source": [
    "# base_tokenizer_dict = recover_vocab_dict(base_tokenizer_json)\n",
    "# add_tokenizer_dict = recover_vocab_dict(add_tokenizer_json)\n",
    "base_tokenizer_dict = dict(base_tokenizer_json['model']['vocab'])\n",
    "add_tokenizer_dict = dict(add_tokenizer_json['model']['vocab'])\n",
    "print(len(base_tokenizer_dict), len(add_tokenizer_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through new vocab and add them\n",
    "# we take the max because averaging actually reduces the negative logprob compared to other tokens\n",
    "# these tokens are those that are shared! there is no reason for them to be penalized\n",
    "# we take the maximum of each and bump them up a small(epsilon amount) \n",
    "# but we don't want them to exceed 0\n",
    "for token in add_tokenizer_dict:\n",
    "    if token in base_tokenizer_dict:\n",
    "        base_tokenizer_dict[token] = min(\n",
    "            0, max(base_tokenizer_dict[token], add_tokenizer_dict[token]) + epsilon\n",
    "        )\n",
    "    else:\n",
    "        base_tokenizer_dict[token] = add_tokenizer_dict[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82051\n"
     ]
    }
   ],
   "source": [
    "# sort by negative logprob, first, then length then vocab\n",
    "sort_key = lambda x: [-x[1],x[0]]\n",
    "# huggingface tokenizers match added/special tokens directly with the ids\n",
    "# so this sorting process can cause a lot of pain\n",
    "# this can be helped by adding sorted(special_tokens) before using this tool.\n",
    "# another alternative is to manually realign the ids\n",
    "sorted_base = sorted(base_tokenizer_dict.items(), key=sort_key)\n",
    "print(len(sorted_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_tokens = list(filter(lambda token:len(token[0])==1,sorted_base))\n",
    "# assert len(single_tokens) == 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65513 -14.957779311964392\n"
     ]
    }
   ],
   "source": [
    "# reconstitute vocabs. only take {target_vocab_size} number of items\n",
    "# if target_vocab_size is None. the slice does nothing\n",
    "\n",
    "merged_vocab = list(map(list,sorted_base))[:target_intermediate_size]\n",
    "#ensure byte tokens are the unlikeliest\n",
    "min_log_prob = min(merged_vocab,key=lambda item:item[1])[1] - epsilon\n",
    "print(len(merged_vocab), min_log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to forcibly remove bytelevel token items\n",
    "\n",
    "# merged_vocab = [item for item in merged_vocab if item[0] not in byte_level_alphabet]\n",
    "# for item in merged_vocab:\n",
    "#    if item[0] in byte_level_alphabet:\n",
    "#        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab item cynical, replaced with À\n",
      "vocab item traditions, replaced with Á\n",
      "vocab item íĻįìĬ¹, replaced with È\n",
      "vocab item unden, replaced with Ô\n",
      "vocab item pearl, replaced with Ý\n",
      "vocab item ìıĺìķĦ, replaced with ß\n",
      "vocab item spawned, replaced with ä\n",
      "vocab item ê·¸ìĥĪ, replaced with ñ\n",
      "vocab item Ġfirearm, replaced with ò\n",
      "vocab item ìľłëĿ¼ìĭľìķ, replaced with ó\n",
      "vocab item íİľëįĶ, replaced with ô\n",
      "vocab item ë£¨íĨł, replaced with õ\n",
      "vocab item Ġtyrann, replaced with ö\n",
      "vocab item íĪ¬ë¥´íģ¬, replaced with ÷\n",
      "vocab item ì½Ķë¸ĮëĿ¼, replaced with ø\n",
      "vocab item Ġtanky, replaced with ù\n",
      "vocab item hesitan, replaced with ú\n",
      "vocab item TDC, replaced with û\n",
      "vocab item ãħĨãĦ±ãĦ´, replaced with ü\n",
      "vocab item ĠACADEMY, replaced with ý\n",
      "vocab item welve, replaced with þ\n",
      "vocab item Ġhips, replaced with ÿ\n",
      "vocab item ange, replaced with ā\n",
      "vocab item tua, replaced with Ă\n",
      "vocab item hinge, replaced with Ą\n",
      "vocab item ëłĪíĥĢ, replaced with ą\n",
      "vocab item ë¯¸ëĭĪìĸ¸, replaced with Ć\n",
      "vocab item íĥĢìĤ°, replaced with ċ\n",
      "vocab item reacting, replaced with Ď\n",
      "vocab item Ġbreakup, replaced with ď\n",
      "vocab item ì¢ħìĹ°, replaced with Đ\n",
      "vocab item foli, replaced with đ\n",
      "vocab item uy, replaced with Ē\n",
      "vocab item Vac, replaced with ē\n",
      "vocab item nevermind, replaced with Ĕ\n",
      "vocab item Widow, replaced with ĕ\n",
      "vocab item Sup, replaced with Ė\n",
      "vocab item ëŀĢëĵľ, replaced with ė\n",
      "vocab item RTT, replaced with Ę\n",
      "vocab item TOUR, replaced with ę\n",
      "vocab item £¨, replaced with Ĝ\n",
      "vocab item íķľêµŃê±°ëŀĺìĨĮ, replaced with ĝ\n",
      "vocab item íĮĮë¥´íİĺ, replaced with Ğ\n",
      "vocab item iego, replaced with ğ\n",
      "vocab item syl, replaced with ġ\n",
      "65513 45\n"
     ]
    }
   ],
   "source": [
    "# here we do 2 things\n",
    "# 1. push any existing byte_level_alphabets into a low negative log prob\n",
    "# 2. add any byte alphabets back into it, but not append but replace (to keep vocab_size)\n",
    "not_included_count = 0\n",
    "for byte in byte_level_alphabet:\n",
    "    included=False\n",
    "    # if byte\n",
    "    for item in merged_vocab:\n",
    "        if byte == item[0]:\n",
    "            item[1] = min_log_prob\n",
    "            included=True\n",
    "            break\n",
    "    # if byte token isnt included (was trimmed)\n",
    "    # add it inplace of an unlikely token that is not a byte token.\n",
    "    if not included:\n",
    "        not_included_count +=1\n",
    "        for item in reversed(merged_vocab):\n",
    "            if item[0] in byte_level_alphabet:\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"vocab item {item[0]}, replaced with {byte}\")\n",
    "                item[0] = byte\n",
    "                item[1] = min_log_prob\n",
    "                break\n",
    "merged_vocab = sorted(merged_vocab, key=sort_key)\n",
    "assert len(merged_vocab)\n",
    "print(len(merged_vocab), not_included_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65513 65513\n"
     ]
    }
   ],
   "source": [
    "print(len(base_tokenizer_json['model']['vocab']), len(merged_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ń', -14.957779311964392]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_tokenizer_json['model']['vocab'] = merged_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(new_tokenizer_path, \"w\") as new_file:\n",
    "    json.dump(base_tokenizer_json, new_file,indent=2,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
